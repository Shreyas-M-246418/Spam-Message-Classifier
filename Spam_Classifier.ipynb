{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv(\"spam.csv\",encoding=\"latin-1\")\n",
    "\n",
    "df.head()\n",
    "\n",
    "df.info()\n",
    "\n",
    "df.drop(columns=['Unnamed: 2','Unnamed: 3','Unnamed: 4'],inplace=True)\n",
    "\n",
    "df.sample(10)\n",
    "\n",
    "df.rename(columns={'v1':'target','v2':'text'},inplace=True)\n",
    "df.sample(10)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder=LabelEncoder()\n",
    "\n",
    "df['target']=encoder.fit_transform(df['target'])\n",
    "\n",
    "df.head()\n",
    "\n",
    "df.isnull().sum()\n",
    "\n",
    "df.duplicated().sum()\n",
    "\n",
    "df=df.drop_duplicates(keep='first')\n",
    "\n",
    "df.duplicated().sum()\n",
    "\n",
    "df['target'].value_counts()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.pie(df['target'].value_counts(),labels=['ham','spam'],autopct=\"%0.2f\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "df['num_char']=df['text'].apply(len)\n",
    "\n",
    "df.head()\n",
    "\n",
    "df['num_words']=df['text'].apply(lambda x:len(nltk.word_tokenize(x)))\n",
    "\n",
    "df.head()\n",
    "\n",
    "df['num_sent']=df['text'].apply(lambda x:len(nltk.sent_tokenize(x)))\n",
    "\n",
    "df.head()\n",
    "\n",
    "df.head()\n",
    "\n",
    "df[['num_char','num_words','num_sent']].describe()\n",
    "\n",
    "df[df['target']==0][['num_char','num_words','num_sent']].describe()\n",
    "\n",
    "df[df['target']==1][['num_char','num_words','num_sent']].describe()\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.histplot(df[df['target']==0]['num_char'])\n",
    "sns.histplot(df[df['target']==1]['num_char'],color='red')\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.histplot(df[df['target']==0]['num_words'])\n",
    "sns.histplot(df[df['target']==1]['num_words'],color='red')\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.histplot(df[df['target']==0]['num_sent'])\n",
    "sns.histplot(df[df['target']==1]['num_sent'],color='red')\n",
    "\n",
    "sns.pairplot(df,hue='target')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "string.punctuation\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps=PorterStemmer()\n",
    "\n",
    "def trans(text):\n",
    "    text=text.lower()\n",
    "    text=nltk.word_tokenize(text)\n",
    "    y=[]\n",
    "    for i in text:\n",
    "        if i.isalnum():\n",
    "            y.append(i)\n",
    "    \n",
    "    text=y[:]\n",
    "    y.clear()\n",
    "    for  i in text:\n",
    "        if i not in stopwords.words('english') and i not in string.punctuation:\n",
    "            y.append(i)\n",
    "    \n",
    "    text=y[:]\n",
    "    y.clear()\n",
    "    \n",
    "    for i in text:\n",
    "        y.append(ps.stem(i))\n",
    "        \n",
    "    return \" \".join(y)\n",
    "\n",
    "df['transformed']=df['text'].apply(trans)\n",
    "\n",
    "df.head()\n",
    "\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "wc=WordCloud(width=600,height=600,min_font_size=10,background_color='white')\n",
    "\n",
    "spam_wc=wc.generate(df[df['target']==1]['transformed'].str.cat(sep=\" \"))\n",
    "plt.imshow(spam_wc)\n",
    "\n",
    "ham_wc=wc.generate(df[df['target']==0]['transformed'].str.cat(sep=\" \"))\n",
    "plt.imshow(spam_wc)\n",
    "\n",
    "spam_corpus=[]\n",
    "for msg in df[df['target']==1]['transformed'].tolist():\n",
    "    for word in msg.split():\n",
    "        spam_corpus.append(word)\n",
    "\n",
    "len(spam_corpus)\n",
    "\n",
    "from collections import Counter\n",
    "pd.DataFrame(Counter(spam_corpus).most_common(30))\n",
    "\n",
    "spam_corpus=[]\n",
    "for msg in df[df['target']==0]['transformed'].tolist():\n",
    "    for word in msg.split():\n",
    "        spam_corpus.append(word)\n",
    "\n",
    "len(spam_corpus)\n",
    "\n",
    "from collections import Counter\n",
    "pd.DataFrame(Counter(spam_corpus).most_common(30))\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer()\n",
    "\n",
    "x=cv.fit_transform(df['transformed']).toarray()\n",
    "\n",
    "x.shape\n",
    "\n",
    "y=df['target'].values\n",
    "\n",
    "y\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=2)\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB\n",
    "\n",
    "gnb=GaussianNB()\n",
    "mnb=MultinomialNB()\n",
    "bnb=BernoulliNB()\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,precision_score\n",
    "\n",
    "gnb.fit(x_train,y_train)\n",
    "y_pred1=gnb.predict(x_test)\n",
    "print(accuracy_score(y_test,y_pred1))\n",
    "print(confusion_matrix(y_test,y_pred1))\n",
    "print(precision_score(y_test,y_pred1))\n",
    "\n",
    "mnb.fit(x_train,y_train)\n",
    "y_pred2=mnb.predict(x_test)\n",
    "print(accuracy_score(y_test,y_pred2))\n",
    "print(confusion_matrix(y_test,y_pred2))\n",
    "print(precision_score(y_test,y_pred2))\n",
    "\n",
    "bnb.fit(x_train,y_train)\n",
    "y_pred3=bnb.predict(x_test)\n",
    "print(accuracy_score(y_test,y_pred3))\n",
    "print(confusion_matrix(y_test,y_pred3))\n",
    "print(precision_score(y_test,y_pred3))\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf=TfidfVectorizer(max_features=3000)\n",
    "\n",
    "x1=tfidf.fit_transform(df['transformed']).toarray()\n",
    "\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "#scaler=MinMaxScaler()\n",
    "#x1=scaler.fit_transform(x1)\n",
    "\n",
    "y1=df['target'].values\n",
    "\n",
    "x_train1,x_test1,y_train1,y_test1=train_test_split(x1,y1,test_size=0.2,random_state=2)\n",
    "\n",
    "gnb1=GaussianNB()\n",
    "mnb1=MultinomialNB()\n",
    "bnb1=BernoulliNB()\n",
    "\n",
    "gnb1.fit(x_train1,y_train1)\n",
    "y_pred11=gnb1.predict(x_test1)\n",
    "print(accuracy_score(y_test1,y_pred11))\n",
    "print(confusion_matrix(y_test1,y_pred11))\n",
    "print(precision_score(y_test1,y_pred11))\n",
    "\n",
    "mnb1.fit(x_train1,y_train1)\n",
    "y_pred21=mnb1.predict(x_test1)\n",
    "print(accuracy_score(y_test1,y_pred21))\n",
    "print(confusion_matrix(y_test1,y_pred21))\n",
    "print(precision_score(y_test1,y_pred21))\n",
    "\n",
    "bnb1.fit(x_train1,y_train1)\n",
    "y_pred31=bnb1.predict(x_test1)\n",
    "print(accuracy_score(y_test1,y_pred31))\n",
    "print(confusion_matrix(y_test1,y_pred31))\n",
    "print(precision_score(y_test1,y_pred31))\n",
    "\n",
    "import pickle\n",
    "pickle.dump(tfidf,open('vectorizer.pkl','wb'))\n",
    "pickle.dump(mnb1,open('mnb1_model.pkl','wb'))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
